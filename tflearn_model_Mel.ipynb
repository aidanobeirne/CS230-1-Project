{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "import numpy as np\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "def allDone():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data loading and preprocessing\n",
    "X = np.genfromtxt('/home/ubuntu/data/GZTAN/Xtrain.txt').T\n",
    "X = X.reshape([-1, 128, 128, 1])\n",
    "Y = np.genfromtxt('/home/ubuntu/data/GZTAN/Ytrain.txt').T\n",
    "Y = Y.reshape([-1,10])\n",
    "testX = np.genfromtxt('/home/ubuntu/data/GZTAN/Xdev.txt').T\n",
    "testX = testX.reshape([-1, 128, 128, 1])\n",
    "testY = np.genfromtxt('/home/ubuntu/data/GZTAN/Ydev.txt').T\n",
    "testY = testY.reshape([-1,10])\n",
    "\n",
    "TestX = np.genfromtxt('/home/ubuntu/data/GZTAN/Xtest.txt').T\n",
    "TestX = TestX.reshape([-1, 128, 128, 1])\n",
    "TestY = np.genfromtxt('/home/ubuntu/data/GZTAN/Ytest.txt').T\n",
    "TestY = TestY.reshape([-1,10])\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#Conv Layer Parameters\n",
    "keep_prob = 1\n",
    "filter_depth = np.array([64,128,256,512])\n",
    "filter_sizes = [2,2,2,2]\n",
    "strides = [1,2,1,2]\n",
    "fully_connected_size = 1024\n",
    "activation = 'Relu'\n",
    "regularizer = 'L2'\n",
    "weights_init = \"Xavier\"\n",
    "weight_decay = 0.0020\n",
    "MaxPoolLayer = ['True','True','True','True']\n",
    "MaxPoolSize  = [2, 2, 2, 2]\n",
    "# Building convolutional network\n",
    "network = input_data(shape=[None, 128, 128, 1], name='input')\n",
    "\n",
    "#Convolutional Layers\n",
    "for i in range(len(filter_sizes)):\n",
    "    network = conv_2d(network, filter_depth[i], filter_sizes[i], strides=strides[i], activation=activation, weights_init=weights_init, regularizer=regularizer, weight_decay=weight_decay)\n",
    "    network = max_pool_2d(network, MaxPoolSize[i])\n",
    "    #network = dropout(network, keep_prob)\n",
    "    network = batch_normalization(network)\n",
    "#Fully Connected Layer\n",
    "network = fully_connected(network, fully_connected_size, activation='Relu')\n",
    "#network = dropout(network, keep_prob)\n",
    "#Output Layer\n",
    "network = fully_connected(network, 10, activation='softmax')\n",
    "network = regression(network, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy', name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tflearn.DNN(network, tensorboard_verbose=1, tensorboard_dir='/home/ubuntu/logs/', best_checkpoint_path='/home/ubuntu/logs', best_val_accuracy = 88.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 19_Stride1_2_1_2_Lambda=0.0020_LR=0.001\n",
      "Log directory: /home/ubuntu/logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 9500\n",
      "Validation samples: 250\n",
      "--\n",
      "Training Step: 1  | time: 23.095s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 3.06717 - val_acc: 0.0880 -- iter: 0512/9500\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m2.12239\u001b[0m\u001b[0m | time: 44.088s\n",
      "| Adam | epoch: 001 | loss: 2.12239 - acc: 0.0791 | val_loss: 3.84486 - val_acc: 0.0880 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m2.20371\u001b[0m\u001b[0m | time: 65.255s\n",
      "| Adam | epoch: 001 | loss: 2.20371 - acc: 0.2317 | val_loss: 3.13465 - val_acc: 0.0960 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m2.00100\u001b[0m\u001b[0m | time: 86.513s\n",
      "| Adam | epoch: 001 | loss: 2.00100 - acc: 0.3113 | val_loss: 2.48302 - val_acc: 0.0800 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.95068\u001b[0m\u001b[0m | time: 107.601s\n",
      "| Adam | epoch: 001 | loss: 1.95068 - acc: 0.2770 | val_loss: 2.40254 - val_acc: 0.0640 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.63552\u001b[0m\u001b[0m | time: 128.786s\n",
      "| Adam | epoch: 001 | loss: 1.63552 - acc: 0.3915 | val_loss: 2.36827 - val_acc: 0.0760 -- iter: 3072/9500\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.49539\u001b[0m\u001b[0m | time: 149.892s\n",
      "| Adam | epoch: 001 | loss: 1.49539 - acc: 0.4601 | val_loss: 2.16283 - val_acc: 0.1800 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.45471\u001b[0m\u001b[0m | time: 170.970s\n",
      "| Adam | epoch: 001 | loss: 1.45471 - acc: 0.4880 | val_loss: 2.07764 - val_acc: 0.3080 -- iter: 4096/9500\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.36742\u001b[0m\u001b[0m | time: 192.672s\n",
      "| Adam | epoch: 001 | loss: 1.36742 - acc: 0.5151 | val_loss: 2.07599 - val_acc: 0.2560 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.28249\u001b[0m\u001b[0m | time: 214.031s\n",
      "| Adam | epoch: 001 | loss: 1.28249 - acc: 0.5437 | val_loss: 2.14971 - val_acc: 0.2480 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.18001\u001b[0m\u001b[0m | time: 234.961s\n",
      "| Adam | epoch: 001 | loss: 1.18001 - acc: 0.5729 | val_loss: 2.00002 - val_acc: 0.3440 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.15798\u001b[0m\u001b[0m | time: 256.143s\n",
      "| Adam | epoch: 001 | loss: 1.15798 - acc: 0.5779 | val_loss: 1.83429 - val_acc: 0.3520 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.16547\u001b[0m\u001b[0m | time: 277.302s\n",
      "| Adam | epoch: 001 | loss: 1.16547 - acc: 0.5830 | val_loss: 1.71149 - val_acc: 0.3920 -- iter: 6656/9500\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.09283\u001b[0m\u001b[0m | time: 298.834s\n",
      "| Adam | epoch: 001 | loss: 1.09283 - acc: 0.6066 | val_loss: 1.69763 - val_acc: 0.4080 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.04262\u001b[0m\u001b[0m | time: 320.101s\n",
      "| Adam | epoch: 001 | loss: 1.04262 - acc: 0.6260 | val_loss: 1.62782 - val_acc: 0.4160 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.02949\u001b[0m\u001b[0m | time: 341.187s\n",
      "| Adam | epoch: 001 | loss: 1.02949 - acc: 0.6330 | val_loss: 1.35326 - val_acc: 0.5080 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.00379\u001b[0m\u001b[0m | time: 363.252s\n",
      "| Adam | epoch: 001 | loss: 1.00379 - acc: 0.6449 | val_loss: 1.33083 - val_acc: 0.5120 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.01879\u001b[0m\u001b[0m | time: 384.453s\n",
      "| Adam | epoch: 001 | loss: 1.01879 - acc: 0.6481 | val_loss: 1.36883 - val_acc: 0.5160 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.97365\u001b[0m\u001b[0m | time: 397.255s\n",
      "| Adam | epoch: 001 | loss: 0.97365 - acc: 0.6599 | val_loss: 1.39194 - val_acc: 0.5000 -- iter: 9500/9500\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.95277\u001b[0m\u001b[0m | time: 12.516s\n",
      "| Adam | epoch: 002 | loss: 0.95277 - acc: 0.6674 | val_loss: 1.27255 - val_acc: 0.5320 -- iter: 0512/9500\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m0.90682\u001b[0m\u001b[0m | time: 33.475s\n",
      "| Adam | epoch: 002 | loss: 0.90682 - acc: 0.6843 | val_loss: 1.14165 - val_acc: 0.5960 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m0.89775\u001b[0m\u001b[0m | time: 54.938s\n",
      "| Adam | epoch: 002 | loss: 0.89775 - acc: 0.6800 | val_loss: 1.13634 - val_acc: 0.5840 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m0.87956\u001b[0m\u001b[0m | time: 76.566s\n",
      "| Adam | epoch: 002 | loss: 0.87956 - acc: 0.6873 | val_loss: 1.28697 - val_acc: 0.5600 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m0.88340\u001b[0m\u001b[0m | time: 98.233s\n",
      "| Adam | epoch: 002 | loss: 0.88340 - acc: 0.6818 | val_loss: 1.23903 - val_acc: 0.5560 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m0.88871\u001b[0m\u001b[0m | time: 119.403s\n",
      "| Adam | epoch: 002 | loss: 0.88871 - acc: 0.6834 | val_loss: 1.05545 - val_acc: 0.6120 -- iter: 3072/9500\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m0.89372\u001b[0m\u001b[0m | time: 141.265s\n",
      "| Adam | epoch: 002 | loss: 0.89372 - acc: 0.6814 | val_loss: 1.03287 - val_acc: 0.6160 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m0.88692\u001b[0m\u001b[0m | time: 162.967s\n",
      "| Adam | epoch: 002 | loss: 0.88692 - acc: 0.6824 | val_loss: 1.09335 - val_acc: 0.6000 -- iter: 4096/9500\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m0.83975\u001b[0m\u001b[0m | time: 184.116s\n",
      "| Adam | epoch: 002 | loss: 0.83975 - acc: 0.6988 | val_loss: 1.05667 - val_acc: 0.6160 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m0.82000\u001b[0m\u001b[0m | time: 206.580s\n",
      "| Adam | epoch: 002 | loss: 0.82000 - acc: 0.7099 | val_loss: 1.07844 - val_acc: 0.6280 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.79257\u001b[0m\u001b[0m | time: 228.355s\n",
      "| Adam | epoch: 002 | loss: 0.79257 - acc: 0.7175 | val_loss: 1.15837 - val_acc: 0.5920 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.78034\u001b[0m\u001b[0m | time: 249.486s\n",
      "| Adam | epoch: 002 | loss: 0.78034 - acc: 0.7223 | val_loss: 1.24401 - val_acc: 0.5160 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.76207\u001b[0m\u001b[0m | time: 270.448s\n",
      "| Adam | epoch: 002 | loss: 0.76207 - acc: 0.7290 | val_loss: 1.13757 - val_acc: 0.6040 -- iter: 6656/9500\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.75048\u001b[0m\u001b[0m | time: 291.617s\n",
      "| Adam | epoch: 002 | loss: 0.75048 - acc: 0.7289 | val_loss: 1.01949 - val_acc: 0.6360 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.72178\u001b[0m\u001b[0m | time: 312.698s\n",
      "| Adam | epoch: 002 | loss: 0.72178 - acc: 0.7414 | val_loss: 1.09749 - val_acc: 0.5920 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.69939\u001b[0m\u001b[0m | time: 333.790s\n",
      "| Adam | epoch: 002 | loss: 0.69939 - acc: 0.7550 | val_loss: 1.12577 - val_acc: 0.5600 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.68268\u001b[0m\u001b[0m | time: 355.140s\n",
      "| Adam | epoch: 002 | loss: 0.68268 - acc: 0.7612 | val_loss: 0.94146 - val_acc: 0.6360 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.68657\u001b[0m\u001b[0m | time: 376.544s\n",
      "| Adam | epoch: 002 | loss: 0.68657 - acc: 0.7636 | val_loss: 0.96138 - val_acc: 0.6400 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.66058\u001b[0m\u001b[0m | time: 397.483s\n",
      "| Adam | epoch: 002 | loss: 0.66058 - acc: 0.7728 | val_loss: 1.08867 - val_acc: 0.6040 -- iter: 9500/9500\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.65681\u001b[0m\u001b[0m | time: 12.788s\n",
      "| Adam | epoch: 003 | loss: 0.65681 - acc: 0.7711 | val_loss: 1.11966 - val_acc: 0.6080 -- iter: 0512/9500\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.63730\u001b[0m\u001b[0m | time: 25.448s\n",
      "| Adam | epoch: 003 | loss: 0.63730 - acc: 0.7843 | val_loss: 0.98014 - val_acc: 0.6280 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.60519\u001b[0m\u001b[0m | time: 46.770s\n",
      "| Adam | epoch: 003 | loss: 0.60519 - acc: 0.8000 | val_loss: 0.89796 - val_acc: 0.6560 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.59519\u001b[0m\u001b[0m | time: 67.989s\n",
      "| Adam | epoch: 003 | loss: 0.59519 - acc: 0.8029 | val_loss: 0.89053 - val_acc: 0.6840 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.59307\u001b[0m\u001b[0m | time: 89.102s\n",
      "| Adam | epoch: 003 | loss: 0.59307 - acc: 0.8053 | val_loss: 0.85663 - val_acc: 0.7160 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.58414\u001b[0m\u001b[0m | time: 110.550s\n",
      "| Adam | epoch: 003 | loss: 0.58414 - acc: 0.8055 | val_loss: 0.86414 - val_acc: 0.6960 -- iter: 3072/9500\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.57513\u001b[0m\u001b[0m | time: 131.834s\n",
      "| Adam | epoch: 003 | loss: 0.57513 - acc: 0.8077 | val_loss: 0.83564 - val_acc: 0.7200 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.57216\u001b[0m\u001b[0m | time: 152.871s\n",
      "| Adam | epoch: 003 | loss: 0.57216 - acc: 0.8082 | val_loss: 0.75333 - val_acc: 0.7520 -- iter: 4096/9500\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.55650\u001b[0m\u001b[0m | time: 174.221s\n",
      "| Adam | epoch: 003 | loss: 0.55650 - acc: 0.8121 | val_loss: 0.66535 - val_acc: 0.7600 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.53471\u001b[0m\u001b[0m | time: 195.405s\n",
      "| Adam | epoch: 003 | loss: 0.53471 - acc: 0.8222 | val_loss: 0.66106 - val_acc: 0.7600 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.51274\u001b[0m\u001b[0m | time: 216.505s\n",
      "| Adam | epoch: 003 | loss: 0.51274 - acc: 0.8293 | val_loss: 0.78394 - val_acc: 0.7080 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.49218\u001b[0m\u001b[0m | time: 237.769s\n",
      "| Adam | epoch: 003 | loss: 0.49218 - acc: 0.8367 | val_loss: 0.92870 - val_acc: 0.6760 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.46388\u001b[0m\u001b[0m | time: 258.993s\n",
      "| Adam | epoch: 003 | loss: 0.46388 - acc: 0.8506 | val_loss: 0.76219 - val_acc: 0.7320 -- iter: 6656/9500\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.45913\u001b[0m\u001b[0m | time: 280.109s\n",
      "| Adam | epoch: 003 | loss: 0.45913 - acc: 0.8513 | val_loss: 0.65425 - val_acc: 0.7600 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.44338\u001b[0m\u001b[0m | time: 301.270s\n",
      "| Adam | epoch: 003 | loss: 0.44338 - acc: 0.8574 | val_loss: 0.73921 - val_acc: 0.7440 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.42635\u001b[0m\u001b[0m | time: 322.467s\n",
      "| Adam | epoch: 003 | loss: 0.42635 - acc: 0.8625 | val_loss: 0.87414 - val_acc: 0.6960 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.41527\u001b[0m\u001b[0m | time: 343.686s\n",
      "| Adam | epoch: 003 | loss: 0.41527 - acc: 0.8662 | val_loss: 0.76162 - val_acc: 0.7360 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.41096\u001b[0m\u001b[0m | time: 364.968s\n",
      "| Adam | epoch: 003 | loss: 0.41096 - acc: 0.8647 | val_loss: 0.73912 - val_acc: 0.7520 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.39628\u001b[0m\u001b[0m | time: 386.171s\n",
      "| Adam | epoch: 003 | loss: 0.39628 - acc: 0.8705 | val_loss: 0.70950 - val_acc: 0.7720 -- iter: 9500/9500\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.38751\u001b[0m\u001b[0m | time: 21.347s\n",
      "| Adam | epoch: 004 | loss: 0.38751 - acc: 0.8746 | val_loss: 0.71984 - val_acc: 0.7640 -- iter: 0512/9500\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.38063\u001b[0m\u001b[0m | time: 33.920s\n",
      "| Adam | epoch: 004 | loss: 0.38063 - acc: 0.8778 | val_loss: 0.65073 - val_acc: 0.7760 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.37068\u001b[0m\u001b[0m | time: 46.674s\n",
      "| Adam | epoch: 004 | loss: 0.37068 - acc: 0.8795 | val_loss: 0.57271 - val_acc: 0.8080 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.35391\u001b[0m\u001b[0m | time: 67.880s\n",
      "| Adam | epoch: 004 | loss: 0.35391 - acc: 0.8865 | val_loss: 0.60747 - val_acc: 0.7880 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.35047\u001b[0m\u001b[0m | time: 89.174s\n",
      "| Adam | epoch: 004 | loss: 0.35047 - acc: 0.8878 | val_loss: 0.53340 - val_acc: 0.8160 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.34997\u001b[0m\u001b[0m | time: 110.167s\n",
      "| Adam | epoch: 004 | loss: 0.34997 - acc: 0.8874 | val_loss: 0.52947 - val_acc: 0.8240 -- iter: 3072/9500\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.34472\u001b[0m\u001b[0m | time: 131.378s\n",
      "| Adam | epoch: 004 | loss: 0.34472 - acc: 0.8873 | val_loss: 0.58443 - val_acc: 0.8120 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.33750\u001b[0m\u001b[0m | time: 152.694s\n",
      "| Adam | epoch: 004 | loss: 0.33750 - acc: 0.8911 | val_loss: 0.57116 - val_acc: 0.8080 -- iter: 4096/9500\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.32922\u001b[0m\u001b[0m | time: 173.676s\n",
      "| Adam | epoch: 004 | loss: 0.32922 - acc: 0.8941 | val_loss: 0.70998 - val_acc: 0.7480 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.32938\u001b[0m\u001b[0m | time: 194.801s\n",
      "| Adam | epoch: 004 | loss: 0.32938 - acc: 0.8939 | val_loss: 0.68799 - val_acc: 0.7680 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.31915\u001b[0m\u001b[0m | time: 215.947s\n",
      "| Adam | epoch: 004 | loss: 0.31915 - acc: 0.8996 | val_loss: 0.56776 - val_acc: 0.8360 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.30336\u001b[0m\u001b[0m | time: 237.002s\n",
      "| Adam | epoch: 004 | loss: 0.30336 - acc: 0.9049 | val_loss: 0.55381 - val_acc: 0.8240 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.29115\u001b[0m\u001b[0m | time: 258.642s\n",
      "| Adam | epoch: 004 | loss: 0.29115 - acc: 0.9080 | val_loss: 0.57450 - val_acc: 0.8160 -- iter: 6656/9500\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.27429\u001b[0m\u001b[0m | time: 279.880s\n",
      "| Adam | epoch: 004 | loss: 0.27429 - acc: 0.9127 | val_loss: 0.59849 - val_acc: 0.8080 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.25939\u001b[0m\u001b[0m | time: 300.920s\n",
      "| Adam | epoch: 004 | loss: 0.25939 - acc: 0.9186 | val_loss: 0.60441 - val_acc: 0.8080 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.24929\u001b[0m\u001b[0m | time: 322.200s\n",
      "| Adam | epoch: 004 | loss: 0.24929 - acc: 0.9217 | val_loss: 0.49514 - val_acc: 0.8400 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.23963\u001b[0m\u001b[0m | time: 343.306s\n",
      "| Adam | epoch: 004 | loss: 0.23963 - acc: 0.9256 | val_loss: 0.48657 - val_acc: 0.8360 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.22784\u001b[0m\u001b[0m | time: 364.536s\n",
      "| Adam | epoch: 004 | loss: 0.22784 - acc: 0.9297 | val_loss: 0.53501 - val_acc: 0.8280 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.22131\u001b[0m\u001b[0m | time: 385.847s\n",
      "| Adam | epoch: 004 | loss: 0.22131 - acc: 0.9324 | val_loss: 0.63644 - val_acc: 0.7880 -- iter: 9500/9500\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.21398\u001b[0m\u001b[0m | time: 21.129s\n",
      "| Adam | epoch: 005 | loss: 0.21398 - acc: 0.9352 | val_loss: 0.59050 - val_acc: 0.7960 -- iter: 0512/9500\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.20864\u001b[0m\u001b[0m | time: 42.328s\n",
      "| Adam | epoch: 005 | loss: 0.20864 - acc: 0.9365 | val_loss: 0.54349 - val_acc: 0.8080 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.20616\u001b[0m\u001b[0m | time: 54.921s\n",
      "| Adam | epoch: 005 | loss: 0.20616 - acc: 0.9356 | val_loss: 0.61221 - val_acc: 0.7760 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.20108\u001b[0m\u001b[0m | time: 67.475s\n",
      "| Adam | epoch: 005 | loss: 0.20108 - acc: 0.9378 | val_loss: 0.49159 - val_acc: 0.8160 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.19276\u001b[0m\u001b[0m | time: 88.512s\n",
      "| Adam | epoch: 005 | loss: 0.19276 - acc: 0.9409 | val_loss: 0.44972 - val_acc: 0.8400 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.18695\u001b[0m\u001b[0m | time: 109.599s\n",
      "| Adam | epoch: 005 | loss: 0.18695 - acc: 0.9441 | val_loss: 0.52676 - val_acc: 0.8280 -- iter: 3072/9500\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.18371\u001b[0m\u001b[0m | time: 131.005s\n",
      "| Adam | epoch: 005 | loss: 0.18371 - acc: 0.9452 | val_loss: 0.54758 - val_acc: 0.8120 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.18182\u001b[0m\u001b[0m | time: 152.149s\n",
      "| Adam | epoch: 005 | loss: 0.18182 - acc: 0.9454 | val_loss: 0.54851 - val_acc: 0.8040 -- iter: 4096/9500\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.18062\u001b[0m\u001b[0m | time: 173.443s\n",
      "| Adam | epoch: 005 | loss: 0.18062 - acc: 0.9458 | val_loss: 0.55155 - val_acc: 0.8160 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.17613\u001b[0m\u001b[0m | time: 194.656s\n",
      "| Adam | epoch: 005 | loss: 0.17613 - acc: 0.9475 | val_loss: 0.48562 - val_acc: 0.8440 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.17539\u001b[0m\u001b[0m | time: 215.746s\n",
      "| Adam | epoch: 005 | loss: 0.17539 - acc: 0.9479 | val_loss: 0.57784 - val_acc: 0.8040 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.17031\u001b[0m\u001b[0m | time: 236.910s\n",
      "| Adam | epoch: 005 | loss: 0.17031 - acc: 0.9507 | val_loss: 0.74124 - val_acc: 0.7640 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.16244\u001b[0m\u001b[0m | time: 257.922s\n",
      "| Adam | epoch: 005 | loss: 0.16244 - acc: 0.9539 | val_loss: 0.63023 - val_acc: 0.8040 -- iter: 6656/9500\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.15357\u001b[0m\u001b[0m | time: 278.989s\n",
      "| Adam | epoch: 005 | loss: 0.15357 - acc: 0.9571 | val_loss: 0.58685 - val_acc: 0.8320 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.14457\u001b[0m\u001b[0m | time: 299.910s\n",
      "| Adam | epoch: 005 | loss: 0.14457 - acc: 0.9603 | val_loss: 0.54696 - val_acc: 0.8320 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.13620\u001b[0m\u001b[0m | time: 321.231s\n",
      "| Adam | epoch: 005 | loss: 0.13620 - acc: 0.9633 | val_loss: 0.46483 - val_acc: 0.8520 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.13004\u001b[0m\u001b[0m | time: 342.580s\n",
      "| Adam | epoch: 005 | loss: 0.13004 - acc: 0.9652 | val_loss: 0.40292 - val_acc: 0.8840 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.12280\u001b[0m\u001b[0m | time: 363.685s\n",
      "| Adam | epoch: 005 | loss: 0.12280 - acc: 0.9681 | val_loss: 0.41448 - val_acc: 0.8720 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.11558\u001b[0m\u001b[0m | time: 385.018s\n",
      "| Adam | epoch: 005 | loss: 0.11558 - acc: 0.9703 | val_loss: 0.44321 - val_acc: 0.8760 -- iter: 9500/9500\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.11147\u001b[0m\u001b[0m | time: 21.163s\n",
      "| Adam | epoch: 006 | loss: 0.11147 - acc: 0.9719 | val_loss: 0.46806 - val_acc: 0.8480 -- iter: 0512/9500\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.10692\u001b[0m\u001b[0m | time: 42.427s\n",
      "| Adam | epoch: 006 | loss: 0.10692 - acc: 0.9735 | val_loss: 0.47867 - val_acc: 0.8600 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.10240\u001b[0m\u001b[0m | time: 63.727s\n",
      "| Adam | epoch: 006 | loss: 0.10240 - acc: 0.9746 | val_loss: 0.42970 - val_acc: 0.8800 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.09890\u001b[0m\u001b[0m | time: 76.403s\n",
      "| Adam | epoch: 006 | loss: 0.09890 - acc: 0.9756 | val_loss: 0.52799 - val_acc: 0.8160 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.09423\u001b[0m\u001b[0m | time: 89.057s\n",
      "| Adam | epoch: 006 | loss: 0.09423 - acc: 0.9770 | val_loss: 0.59029 - val_acc: 0.8120 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.08862\u001b[0m\u001b[0m | time: 110.356s\n",
      "| Adam | epoch: 006 | loss: 0.08862 - acc: 0.9789 | val_loss: 0.40229 - val_acc: 0.8760 -- iter: 3072/9500\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.09274\u001b[0m\u001b[0m | time: 131.496s\n",
      "| Adam | epoch: 006 | loss: 0.09274 - acc: 0.9765 | val_loss: 0.53040 - val_acc: 0.8360 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.09123\u001b[0m\u001b[0m | time: 152.710s\n",
      "| Adam | epoch: 006 | loss: 0.09123 - acc: 0.9767 | val_loss: 0.57577 - val_acc: 0.8240 -- iter: 4096/9500\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.09142\u001b[0m\u001b[0m | time: 174.011s\n",
      "| Adam | epoch: 006 | loss: 0.09142 - acc: 0.9767 | val_loss: 0.54597 - val_acc: 0.8120 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.08902\u001b[0m\u001b[0m | time: 195.281s\n",
      "| Adam | epoch: 006 | loss: 0.08902 - acc: 0.9777 | val_loss: 0.57455 - val_acc: 0.8120 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.08745\u001b[0m\u001b[0m | time: 216.259s\n",
      "| Adam | epoch: 006 | loss: 0.08745 - acc: 0.9785 | val_loss: 0.58152 - val_acc: 0.8200 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.08536\u001b[0m\u001b[0m | time: 237.461s\n",
      "| Adam | epoch: 006 | loss: 0.08536 - acc: 0.9793 | val_loss: 0.55564 - val_acc: 0.8240 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.07995\u001b[0m\u001b[0m | time: 258.760s\n",
      "| Adam | epoch: 006 | loss: 0.07995 - acc: 0.9806 | val_loss: 0.51899 - val_acc: 0.8320 -- iter: 6656/9500\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.07810\u001b[0m\u001b[0m | time: 279.864s\n",
      "| Adam | epoch: 006 | loss: 0.07810 - acc: 0.9808 | val_loss: 0.49579 - val_acc: 0.8680 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.07614\u001b[0m\u001b[0m | time: 301.104s\n",
      "| Adam | epoch: 006 | loss: 0.07614 - acc: 0.9817 | val_loss: 0.50150 - val_acc: 0.8520 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.07261\u001b[0m\u001b[0m | time: 322.246s\n",
      "| Adam | epoch: 006 | loss: 0.07261 - acc: 0.9826 | val_loss: 0.52989 - val_acc: 0.8440 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.06770\u001b[0m\u001b[0m | time: 343.657s\n",
      "| Adam | epoch: 006 | loss: 0.06770 - acc: 0.9839 | val_loss: 0.57017 - val_acc: 0.8240 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.06462\u001b[0m\u001b[0m | time: 364.727s\n",
      "| Adam | epoch: 006 | loss: 0.06462 - acc: 0.9852 | val_loss: 0.52399 - val_acc: 0.8320 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.06386\u001b[0m\u001b[0m | time: 386.009s\n",
      "| Adam | epoch: 006 | loss: 0.06386 - acc: 0.9847 | val_loss: 0.48139 - val_acc: 0.8480 -- iter: 9500/9500\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.06119\u001b[0m\u001b[0m | time: 21.214s\n",
      "| Adam | epoch: 007 | loss: 0.06119 - acc: 0.9856 | val_loss: 0.41856 - val_acc: 0.8680 -- iter: 0512/9500\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.05920\u001b[0m\u001b[0m | time: 42.402s\n",
      "| Adam | epoch: 007 | loss: 0.05920 - acc: 0.9855 | val_loss: 0.36763 - val_acc: 0.8760 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.05692\u001b[0m\u001b[0m | time: 63.638s\n",
      "| Adam | epoch: 007 | loss: 0.05692 - acc: 0.9860 | val_loss: 0.37563 - val_acc: 0.8760 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.05515\u001b[0m\u001b[0m | time: 84.766s\n",
      "| Adam | epoch: 007 | loss: 0.05515 - acc: 0.9864 | val_loss: 0.44643 - val_acc: 0.8520 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.05251\u001b[0m\u001b[0m | time: 97.485s\n",
      "| Adam | epoch: 007 | loss: 0.05251 - acc: 0.9874 | val_loss: 0.53893 - val_acc: 0.8120 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.05467\u001b[0m\u001b[0m | time: 110.069s\n",
      "| Adam | epoch: 007 | loss: 0.05467 - acc: 0.9858 | val_loss: 0.64207 - val_acc: 0.8040 -- iter: 3072/9500\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.05200\u001b[0m\u001b[0m | time: 131.399s\n",
      "| Adam | epoch: 007 | loss: 0.05200 - acc: 0.9872 | val_loss: 0.45106 - val_acc: 0.8600 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.05649\u001b[0m\u001b[0m | time: 152.529s\n",
      "| Adam | epoch: 007 | loss: 0.05649 - acc: 0.9862 | val_loss: 0.69386 - val_acc: 0.7920 -- iter: 4096/9500\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.05486\u001b[0m\u001b[0m | time: 173.876s\n",
      "| Adam | epoch: 007 | loss: 0.05486 - acc: 0.9874 | val_loss: 0.44204 - val_acc: 0.8520 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.05727\u001b[0m\u001b[0m | time: 195.050s\n",
      "| Adam | epoch: 007 | loss: 0.05727 - acc: 0.9861 | val_loss: 0.53693 - val_acc: 0.8280 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.05549\u001b[0m\u001b[0m | time: 216.302s\n",
      "| Adam | epoch: 007 | loss: 0.05549 - acc: 0.9869 | val_loss: 0.56276 - val_acc: 0.8240 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.05553\u001b[0m\u001b[0m | time: 237.411s\n",
      "| Adam | epoch: 007 | loss: 0.05553 - acc: 0.9868 | val_loss: 0.52673 - val_acc: 0.8360 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.06075\u001b[0m\u001b[0m | time: 258.552s\n",
      "| Adam | epoch: 007 | loss: 0.06075 - acc: 0.9850 | val_loss: 0.70857 - val_acc: 0.8200 -- iter: 6656/9500\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.06466\u001b[0m\u001b[0m | time: 279.708s\n",
      "| Adam | epoch: 007 | loss: 0.06466 - acc: 0.9859 | val_loss: 0.80682 - val_acc: 0.8040 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.06250\u001b[0m\u001b[0m | time: 300.871s\n",
      "| Adam | epoch: 007 | loss: 0.06250 - acc: 0.9860 | val_loss: 0.60299 - val_acc: 0.8320 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.06276\u001b[0m\u001b[0m | time: 321.910s\n",
      "| Adam | epoch: 007 | loss: 0.06276 - acc: 0.9852 | val_loss: 0.61971 - val_acc: 0.8120 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.06033\u001b[0m\u001b[0m | time: 342.996s\n",
      "| Adam | epoch: 007 | loss: 0.06033 - acc: 0.9859 | val_loss: 0.78207 - val_acc: 0.7680 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.05863\u001b[0m\u001b[0m | time: 364.086s\n",
      "| Adam | epoch: 007 | loss: 0.05863 - acc: 0.9862 | val_loss: 0.84380 - val_acc: 0.7600 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.05752\u001b[0m\u001b[0m | time: 385.338s\n",
      "| Adam | epoch: 007 | loss: 0.05752 - acc: 0.9862 | val_loss: 0.75239 - val_acc: 0.7960 -- iter: 9500/9500\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.05635\u001b[0m\u001b[0m | time: 21.084s\n",
      "| Adam | epoch: 008 | loss: 0.05635 - acc: 0.9860 | val_loss: 0.63275 - val_acc: 0.8440 -- iter: 0512/9500\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.05424\u001b[0m\u001b[0m | time: 42.228s\n",
      "| Adam | epoch: 008 | loss: 0.05424 - acc: 0.9868 | val_loss: 0.52944 - val_acc: 0.8680 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.05338\u001b[0m\u001b[0m | time: 63.166s\n",
      "| Adam | epoch: 008 | loss: 0.05338 - acc: 0.9864 | val_loss: 0.49543 - val_acc: 0.8640 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.05121\u001b[0m\u001b[0m | time: 84.423s\n",
      "| Adam | epoch: 008 | loss: 0.05121 - acc: 0.9868 | val_loss: 0.46791 - val_acc: 0.8600 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.05047\u001b[0m\u001b[0m | time: 105.315s\n",
      "| Adam | epoch: 008 | loss: 0.05047 - acc: 0.9865 | val_loss: 0.49871 - val_acc: 0.8320 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.05030\u001b[0m\u001b[0m | time: 118.145s\n",
      "| Adam | epoch: 008 | loss: 0.05030 - acc: 0.9859 | val_loss: 0.56280 - val_acc: 0.8120 -- iter: 3072/9500\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.05057\u001b[0m\u001b[0m | time: 130.700s\n",
      "| Adam | epoch: 008 | loss: 0.05057 - acc: 0.9866 | val_loss: 0.57346 - val_acc: 0.8040 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.04907\u001b[0m\u001b[0m | time: 151.868s\n",
      "| Adam | epoch: 008 | loss: 0.04907 - acc: 0.9873 | val_loss: 0.50560 - val_acc: 0.8360 -- iter: 4096/9500\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.05131\u001b[0m\u001b[0m | time: 173.091s\n",
      "| Adam | epoch: 008 | loss: 0.05131 - acc: 0.9872 | val_loss: 0.50256 - val_acc: 0.8520 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.05032\u001b[0m\u001b[0m | time: 194.226s\n",
      "| Adam | epoch: 008 | loss: 0.05032 - acc: 0.9875 | val_loss: 0.42018 - val_acc: 0.8520 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.05054\u001b[0m\u001b[0m | time: 215.456s\n",
      "| Adam | epoch: 008 | loss: 0.05054 - acc: 0.9875 | val_loss: 0.42709 - val_acc: 0.8480 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.04836\u001b[0m\u001b[0m | time: 236.647s\n",
      "| Adam | epoch: 008 | loss: 0.04836 - acc: 0.9882 | val_loss: 0.46131 - val_acc: 0.8520 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.04665\u001b[0m\u001b[0m | time: 257.601s\n",
      "| Adam | epoch: 008 | loss: 0.04665 - acc: 0.9890 | val_loss: 0.50813 - val_acc: 0.8440 -- iter: 6656/9500\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.04461\u001b[0m\u001b[0m | time: 278.801s\n",
      "| Adam | epoch: 008 | loss: 0.04461 - acc: 0.9893 | val_loss: 0.54164 - val_acc: 0.8440 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.04739\u001b[0m\u001b[0m | time: 299.958s\n",
      "| Adam | epoch: 008 | loss: 0.04739 - acc: 0.9896 | val_loss: 1.91305 - val_acc: 0.6400 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.04457\u001b[0m\u001b[0m | time: 321.235s\n",
      "| Adam | epoch: 008 | loss: 0.04457 - acc: 0.9903 | val_loss: 0.63145 - val_acc: 0.8200 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.04448\u001b[0m\u001b[0m | time: 342.454s\n",
      "| Adam | epoch: 008 | loss: 0.04448 - acc: 0.9904 | val_loss: 0.67782 - val_acc: 0.8120 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.04155\u001b[0m\u001b[0m | time: 363.591s\n",
      "| Adam | epoch: 008 | loss: 0.04155 - acc: 0.9912 | val_loss: 0.66405 - val_acc: 0.8080 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.03979\u001b[0m\u001b[0m | time: 384.899s\n",
      "| Adam | epoch: 008 | loss: 0.03979 - acc: 0.9913 | val_loss: 0.65127 - val_acc: 0.8200 -- iter: 9500/9500\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.03763\u001b[0m\u001b[0m | time: 21.297s\n",
      "| Adam | epoch: 009 | loss: 0.03763 - acc: 0.9918 | val_loss: 0.70966 - val_acc: 0.8080 -- iter: 0512/9500\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.03566\u001b[0m\u001b[0m | time: 42.445s\n",
      "| Adam | epoch: 009 | loss: 0.03566 - acc: 0.9920 | val_loss: 0.74387 - val_acc: 0.8080 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.03360\u001b[0m\u001b[0m | time: 63.744s\n",
      "| Adam | epoch: 009 | loss: 0.03360 - acc: 0.9926 | val_loss: 0.71053 - val_acc: 0.8120 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.03282\u001b[0m\u001b[0m | time: 84.977s\n",
      "| Adam | epoch: 009 | loss: 0.03282 - acc: 0.9930 | val_loss: 0.65452 - val_acc: 0.8360 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.03510\u001b[0m\u001b[0m | time: 106.172s\n",
      "| Adam | epoch: 009 | loss: 0.03510 - acc: 0.9931 | val_loss: 0.60547 - val_acc: 0.8480 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.03379\u001b[0m\u001b[0m | time: 127.503s\n",
      "| Adam | epoch: 009 | loss: 0.03379 - acc: 0.9930 | val_loss: 0.53615 - val_acc: 0.8600 -- iter: 3072/9500\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.03362\u001b[0m\u001b[0m | time: 140.136s\n",
      "| Adam | epoch: 009 | loss: 0.03362 - acc: 0.9931 | val_loss: 0.50876 - val_acc: 0.8640 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.03197\u001b[0m\u001b[0m | time: 152.833s\n",
      "| Adam | epoch: 009 | loss: 0.03197 - acc: 0.9934 | val_loss: 0.50788 - val_acc: 0.8560 -- iter: 4096/9500\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.02954\u001b[0m\u001b[0m | time: 174.019s\n",
      "| Adam | epoch: 009 | loss: 0.02954 - acc: 0.9941 | val_loss: 0.52205 - val_acc: 0.8480 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.03038\u001b[0m\u001b[0m | time: 195.196s\n",
      "| Adam | epoch: 009 | loss: 0.03038 - acc: 0.9941 | val_loss: 0.58061 - val_acc: 0.8400 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.02989\u001b[0m\u001b[0m | time: 216.514s\n",
      "| Adam | epoch: 009 | loss: 0.02989 - acc: 0.9941 | val_loss: 0.66135 - val_acc: 0.8200 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.02913\u001b[0m\u001b[0m | time: 237.638s\n",
      "| Adam | epoch: 009 | loss: 0.02913 - acc: 0.9939 | val_loss: 0.67151 - val_acc: 0.8200 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.02796\u001b[0m\u001b[0m | time: 258.523s\n",
      "| Adam | epoch: 009 | loss: 0.02796 - acc: 0.9943 | val_loss: 0.62098 - val_acc: 0.8280 -- iter: 6656/9500\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.02932\u001b[0m\u001b[0m | time: 279.829s\n",
      "| Adam | epoch: 009 | loss: 0.02932 - acc: 0.9941 | val_loss: 0.57502 - val_acc: 0.8560 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.02798\u001b[0m\u001b[0m | time: 300.962s\n",
      "| Adam | epoch: 009 | loss: 0.02798 - acc: 0.9945 | val_loss: 0.56479 - val_acc: 0.8680 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.02774\u001b[0m\u001b[0m | time: 322.104s\n",
      "| Adam | epoch: 009 | loss: 0.02774 - acc: 0.9949 | val_loss: 0.55698 - val_acc: 0.8640 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.02632\u001b[0m\u001b[0m | time: 343.382s\n",
      "| Adam | epoch: 009 | loss: 0.02632 - acc: 0.9952 | val_loss: 0.54528 - val_acc: 0.8680 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.02575\u001b[0m\u001b[0m | time: 364.602s\n",
      "| Adam | epoch: 009 | loss: 0.02575 - acc: 0.9953 | val_loss: 0.53963 - val_acc: 0.8720 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.02648\u001b[0m\u001b[0m | time: 385.789s\n",
      "| Adam | epoch: 009 | loss: 0.02648 - acc: 0.9952 | val_loss: 0.55038 - val_acc: 0.8840 -- iter: 9500/9500\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.02496\u001b[0m\u001b[0m | time: 21.245s\n",
      "| Adam | epoch: 010 | loss: 0.02496 - acc: 0.9954 | val_loss: 0.58745 - val_acc: 0.8640 -- iter: 0512/9500\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.02371\u001b[0m\u001b[0m | time: 42.543s\n",
      "| Adam | epoch: 010 | loss: 0.02371 - acc: 0.9953 | val_loss: 0.64061 - val_acc: 0.8640 -- iter: 1024/9500\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.02209\u001b[0m\u001b[0m | time: 63.582s\n",
      "| Adam | epoch: 010 | loss: 0.02209 - acc: 0.9958 | val_loss: 0.68860 - val_acc: 0.8440 -- iter: 1536/9500\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.02099\u001b[0m\u001b[0m | time: 84.795s\n",
      "| Adam | epoch: 010 | loss: 0.02099 - acc: 0.9960 | val_loss: 0.70000 - val_acc: 0.8280 -- iter: 2048/9500\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 105.994s\n",
      "| Adam | epoch: 010 | loss: 0.02092 - acc: 0.9962 | val_loss: 0.65441 - val_acc: 0.8560 -- iter: 2560/9500\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.02101\u001b[0m\u001b[0m | time: 127.307s\n",
      "| Adam | epoch: 010 | loss: 0.02101 - acc: 0.9962 | val_loss: 0.60074 - val_acc: 0.8680 -- iter: 3072/9500\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.02053\u001b[0m\u001b[0m | time: 148.617s\n",
      "| Adam | epoch: 010 | loss: 0.02053 - acc: 0.9962 | val_loss: 0.55261 - val_acc: 0.8680 -- iter: 3584/9500\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.01999\u001b[0m\u001b[0m | time: 161.338s\n",
      "| Adam | epoch: 010 | loss: 0.01999 - acc: 0.9962 | val_loss: 0.50036 - val_acc: 0.8760 -- iter: 4096/9500\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 173.968s\n",
      "| Adam | epoch: 010 | loss: 0.01883 - acc: 0.9966 | val_loss: 0.46506 - val_acc: 0.8640 -- iter: 4608/9500\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.01752\u001b[0m\u001b[0m | time: 195.343s\n",
      "| Adam | epoch: 010 | loss: 0.01752 - acc: 0.9969 | val_loss: 0.46274 - val_acc: 0.8760 -- iter: 5120/9500\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.01780\u001b[0m\u001b[0m | time: 216.536s\n",
      "| Adam | epoch: 010 | loss: 0.01780 - acc: 0.9970 | val_loss: 0.49117 - val_acc: 0.8680 -- iter: 5632/9500\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.01837\u001b[0m\u001b[0m | time: 237.822s\n",
      "| Adam | epoch: 010 | loss: 0.01837 - acc: 0.9971 | val_loss: 0.52421 - val_acc: 0.8480 -- iter: 6144/9500\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.01756\u001b[0m\u001b[0m | time: 258.977s\n",
      "| Adam | epoch: 010 | loss: 0.01756 - acc: 0.9972 | val_loss: 0.55100 - val_acc: 0.8400 -- iter: 6656/9500\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.01744\u001b[0m\u001b[0m | time: 280.156s\n",
      "| Adam | epoch: 010 | loss: 0.01744 - acc: 0.9973 | val_loss: 0.55228 - val_acc: 0.8440 -- iter: 7168/9500\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.01676\u001b[0m\u001b[0m | time: 301.211s\n",
      "| Adam | epoch: 010 | loss: 0.01676 - acc: 0.9974 | val_loss: 0.54400 - val_acc: 0.8480 -- iter: 7680/9500\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 322.434s\n",
      "| Adam | epoch: 010 | loss: 0.01731 - acc: 0.9972 | val_loss: 0.51290 - val_acc: 0.8520 -- iter: 8192/9500\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.01946\u001b[0m\u001b[0m | time: 343.533s\n",
      "| Adam | epoch: 010 | loss: 0.01946 - acc: 0.9971 | val_loss: 0.48747 - val_acc: 0.8680 -- iter: 8704/9500\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.01799\u001b[0m\u001b[0m | time: 364.689s\n",
      "| Adam | epoch: 010 | loss: 0.01799 - acc: 0.9974 | val_loss: 0.46292 - val_acc: 0.8680 -- iter: 9216/9500\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.01689\u001b[0m\u001b[0m | time: 385.827s\n",
      "| Adam | epoch: 010 | loss: 0.01689 - acc: 0.9975 | val_loss: 0.45803 - val_acc: 0.8680 -- iter: 9500/9500\n",
      "--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RID ='19_Stride1_2_1_2_Lambda=0.0020_LR=0.001'\n",
    "model.fit({'input': X}, {'target': Y}, n_epoch=10, validation_set=({'input': testX}, {'target': testY}),batch_size = 512, snapshot_step = 1, show_metric=True, run_id=RID)\n",
    "allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/ubuntu/88.8_Repeat16_Stride1_2_1_2_Lambda=0.0020.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"88.8_Repeat16_Stride1_2_1_2_Lambda=0.0020.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predict = model.predict(TestX)\n",
    "Predict = Predict.argmax(axis=1)\n",
    "np.sum(Predict == TestY.argmax(axis=1))/float(len(TestY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
